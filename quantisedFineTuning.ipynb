{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edfc5af7-02a7-4e80-ae59-289d62c0a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from huggingface_hub import notebook_login\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c5f047-1ef8-439c-9b37-36fe1a53021b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lvoh8zf8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.017 MB of 0.017 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/ajaysp235-zephyra/huggingface/runs/lvoh8zf8' target=\"_blank\">https://wandb.ai/ajaysp235-zephyra/huggingface/runs/lvoh8zf8</a><br/> View project at: <a href='https://wandb.ai/ajaysp235-zephyra/huggingface' target=\"_blank\">https://wandb.ai/ajaysp235-zephyra/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241104_054726-lvoh8zf8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lvoh8zf8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/root/finetune/wandb/run-20241104_055320-nmrz2w6w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajaysp235-zephyra/medical-berta/runs/nmrz2w6w' target=\"_blank\">medmcqa-finetuning</a></strong> to <a href='https://wandb.ai/ajaysp235-zephyra/medical-berta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajaysp235-zephyra/medical-berta' target=\"_blank\">https://wandb.ai/ajaysp235-zephyra/medical-berta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajaysp235-zephyra/medical-berta/runs/nmrz2w6w' target=\"_blank\">https://wandb.ai/ajaysp235-zephyra/medical-berta/runs/nmrz2w6w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ajaysp235-zephyra/medical-berta/runs/nmrz2w6w?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5480022f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"medical-berta\", \n",
    "          name=\"medmcqa-finetuning\",\n",
    "          config={\n",
    "              \"base_model\": \"microsoft/deberta-v3-small\",  # Small but powerful model\n",
    "              \"dataset\": \"openlifescienceai/medmcqa\",\n",
    "              \"learning_rate\": 3e-4,\n",
    "              \"batch_size\": 8,\n",
    "              \"num_epochs\": 3\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db963ef-5a2e-4a58-8b79-c05f7b86cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_and_preprocess_data(tokenizer):\n",
    "    \"\"\"Load and preprocess the MedMCQA dataset.\"\"\"\n",
    "    dataset = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "    \n",
    "    def format_example(example):\n",
    "        return f\"\"\"Question: {example['question']}\n",
    "A) {example['opa']}\n",
    "B) {example['opb']}\n",
    "C) {example['opc']}\n",
    "D) {example['opd']}\"\"\"\n",
    "    \n",
    "    # Prepare train dataset\n",
    "    train_texts = [format_example(ex) for ex in dataset['train']]\n",
    "    train_labels = [ex['cop'] for ex in dataset['train']]\n",
    "    \n",
    "    # Prepare validation dataset\n",
    "    val_texts = [format_example(ex) for ex in dataset['validation']]\n",
    "    val_labels = [ex['cop'] for ex in dataset['validation']]\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Create dataset objects\n",
    "    train_dataset = MedicalDataset(train_encodings, train_labels)\n",
    "    val_dataset = MedicalDataset(val_encodings, val_labels)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def evaluate_medical_performance(model, tokenizer, dataset):\n",
    "    \"\"\"Evaluate model performance on medical queries.\"\"\"\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.cuda() for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].cuda()\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a66b936-372d-4aa2-aaec-2ec357fa103d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Load base model and tokenizer\n",
    "    model_name = wandb.config.base_model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Configure quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load model with quantization config\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=4,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    \n",
    "    # 2. Log initial model size\n",
    "    def get_model_size(model):\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        return size_all_mb\n",
    "    \n",
    "    initial_size = get_model_size(model)\n",
    "    wandb.log({\"model_size_before_quantization\": initial_size})\n",
    "    \n",
    "    # 3. Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # 4. Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Log quantized model size\n",
    "    quantized_size = get_model_size(model)\n",
    "    wandb.log({\"model_size_after_quantization\": quantized_size})\n",
    "    \n",
    "    # 5. Load and preprocess data\n",
    "    train_dataset, val_dataset = load_and_preprocess_data(tokenizer)\n",
    "    \n",
    "    # 6. Evaluate initial performance on medical queries\n",
    "    initial_accuracy, _ = evaluate_medical_performance(model, tokenizer, val_dataset)\n",
    "    wandb.log({\"initial_medical_accuracy\": initial_accuracy})\n",
    "    \n",
    "    # 7. Configure training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=wandb.config.learning_rate,\n",
    "        per_device_train_batch_size=wandb.config.batch_size,\n",
    "        per_device_eval_batch_size=wandb.config.batch_size,\n",
    "        num_train_epochs=wandb.config.num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "    \n",
    "    # 8. Create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "    \n",
    "    # 9. Train model\n",
    "    trainer.train()\n",
    "    \n",
    "    # 10. Evaluate final performance on medical queries\n",
    "    final_accuracy, predictions = evaluate_medical_performance(model, tokenizer, val_dataset)\n",
    "    wandb.log({\n",
    "        \"final_medical_accuracy\": final_accuracy,\n",
    "        \"accuracy_improvement\": final_accuracy - initial_accuracy\n",
    "    })\n",
    "    \n",
    "    # 11. Log confusion matrix\n",
    "    wandb.log({\n",
    "        \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=[example['labels'].item() for example in val_dataset],\n",
    "            preds=predictions,\n",
    "            class_names=[\"A\", \"B\", \"C\", \"D\"]\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    # 12. Save final model\n",
    "    model.save_pretrained(\"./final_model\")\n",
    "    tokenizer.save_pretrained(\"./final_model\")\n",
    "    \n",
    "    # Close wandb run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35fe8497-037f-41ec-a3c0-dc4d9d3e02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68559' max='68559' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68559/68559 1:36:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>1.371592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.368000</td>\n",
       "      <td>1.360901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.362785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.032 MB of 0.032 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_improvement</td><td>▁</td></tr><tr><td>eval/loss</td><td>█▁▂</td></tr><tr><td>eval/runtime</td><td>▂█▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▁█</td></tr><tr><td>eval/steps_per_second</td><td>▇▁█</td></tr><tr><td>final_medical_accuracy</td><td>▁</td></tr><tr><td>initial_medical_accuracy</td><td>█▁▁</td></tr><tr><td>model_size_after_quantization</td><td>▁▁▁▁</td></tr><tr><td>model_size_before_quantization</td><td>▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▂▃▂▂▄▃▄▃▃▂▃▃▃▂▂▄▂▂▂▂▃▃▁▂▃▂▂▃▁▃▁▂▂▁▃▃▄▃▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▇██▆▆▅▅▄▃▅▃▄▃▃▃▃▃▃▄▃▃▁▄▃▂▃▁▃▂▂▂▃▃▁▂▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_improvement</td><td>0.11236</td></tr><tr><td>eval/loss</td><td>1.36279</td></tr><tr><td>eval/runtime</td><td>13.649</td></tr><tr><td>eval/samples_per_second</td><td>306.469</td></tr><tr><td>eval/steps_per_second</td><td>38.318</td></tr><tr><td>final_medical_accuracy</td><td>0.33349</td></tr><tr><td>initial_medical_accuracy</td><td>0.22113</td></tr><tr><td>model_size_after_quantization</td><td>398.71976</td></tr><tr><td>model_size_before_quantization</td><td>209.05909</td></tr><tr><td>total_flos</td><td>5.615687522610288e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>68559</td></tr><tr><td>train/grad_norm</td><td>1.31312</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.356</td></tr><tr><td>train_loss</td><td>1.37526</td></tr><tr><td>train_runtime</td><td>5793.3297</td></tr><tr><td>train_samples_per_second</td><td>94.672</td></tr><tr><td>train_steps_per_second</td><td>11.834</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">medmcqa-finetuning</strong> at: <a href='https://wandb.ai/ajaysp235-zephyra/medical-berta/runs/nmrz2w6w' target=\"_blank\">https://wandb.ai/ajaysp235-zephyra/medical-berta/runs/nmrz2w6w</a><br/> View project at: <a href='https://wandb.ai/ajaysp235-zephyra/medical-berta' target=\"_blank\">https://wandb.ai/ajaysp235-zephyra/medical-berta</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241104_055320-nmrz2w6w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd133d-75b6-4615-b311-9de4599325af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
